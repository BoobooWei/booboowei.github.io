<!DOCTYPE html>
<html lang="en">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8">
  <title>Hadoop Install & wordcount test | BoobooWei</title>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Canonical links -->
  <link rel="canonical" href="http://www.toberoot.com/database/nosql/hadoop/02_operation/hadoopInstall">
  <!-- Alternate links -->
  
    
      
    
    <link rel="alternate" hreflang="x-default" href="http://www.toberoot.com/database/nosql/hadoop/02_operation/hadoopInstall" />
  
  <!-- Icon -->
  <link rel="apple-touch-icon" sizes="57x57" href="../../../../icon/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="114x114" href="../../../../icon/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="72x72" href="../../../../icon/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="144x144" href="../../../../icon/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="60x60" href="../../../../icon/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="120x120" href="../../../../icon/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="76x76" href="../../../../icon/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="152x152" href="../../../../icon/apple-touch-icon-152x152.png">
  <link rel="icon" type="image/png" href="../../../../icon/favicon-196x196.png" sizes="196x196">
  <link rel="icon" type="image/png" href="../../../../icon/favicon-160x160.png" sizes="160x160">
  <link rel="icon" type="image/png" href="../../../../icon/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="../../../../icon/favicon-16x16.png" sizes="16x16">
  <link rel="icon" type="image/png" href="../../../../icon/favicon-32x32.png" sizes="32x32">
  <meta name="msapplication-TileColor" content="#2f83cd">
  <meta name="msapplication-TileImage" content="../../../../icon/mstile-144x144.png">
  <!-- CSS -->
  <!-- build:css build/css/navy.css -->
  
<link rel="stylesheet" href="../../../../css/navy.css">

  <!-- endbuild -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-lato@0.0.75/index.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css">
  <!-- RSS -->
  <link rel="alternate" href="../../../../atom.xml" title="BoobooWei" type="application/atom+xml">
  <!-- Open Graph -->
  <meta name="description" content="环境介绍OS:rhel7.2 mastera 172.25.0.11 nameserver(8020 50070) resourcemanager（8032 8030 8088 8031 8033 ）jobhistory(10020 19888) slavea 172.25.0.13 datanode nodemanager slaveb 172.25.0.14 datanode nodemana">
<meta property="og:type" content="website">
<meta property="og:title" content="Hadoop Install &amp; wordcount test">
<meta property="og:url" content="http://www.toberoot.com/database/nosql/hadoop/02_operation/hadoopInstall">
<meta property="og:site_name" content="BoobooWei">
<meta property="og:description" content="环境介绍OS:rhel7.2 mastera 172.25.0.11 nameserver(8020 50070) resourcemanager（8032 8030 8088 8031 8033 ）jobhistory(10020 19888) slavea 172.25.0.13 datanode nodemanager slaveb 172.25.0.14 datanode nodemana">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://www.toberoot.com/icon/og-image-wide.png">
<meta property="article:published_time" content="2025-10-22T08:53:55.880Z">
<meta property="article:modified_time" content="2025-10-22T08:49:36.787Z">
<meta property="article:author" content="魏亚萍">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.toberoot.com/icon/twitter-summary.png">
<meta property="fb:admins" content="100000247608790">
  <!-- Google Analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-48498357-3', 'auto');
  ga('send', 'pageview');
</script>

  <!-- Algolia -->
  <link rel="preconnect" href="https://-dsn.algolia.net" crossorigin />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css@3">
<meta name="generator" content="Hexo 8.0.0"></head>

<body>
  <div id="container">
    <header id="header" class="wrapper">
  <div id="header-inner" class="inner">
    <h1 id="logo-wrap">
      <a href="../../../../" id="logo">Hexo</a>
    </h1>
    <nav id="main-nav">
      <a href="../../../../docs/" class="main-nav-link">Hexo</a><a href="../../../../news/" class="main-nav-link">News</a><a href="../../../../api/" class="main-nav-link">MySQL8.0</a><a href="../../../../database/" class="main-nav-link">Database</a><a href="../../../../linux/" class="main-nav-link">Linux</a><a href="../../../../cloud/" class="main-nav-link">Cloud</a><a href="../../../../singapore/" class="main-nav-link">Singapore</a><a href="../../../../amy/" class="main-nav-link">AMY</a><a href="../../../../about/" class="main-nav-link">About</a>
      <a target="_blank" rel="noopener" href="https://github.com/BoobooWei" class="main-nav-link"><i class="fa fa-github-alt"></i></a>

      <div id="search-input-wrap">
        <div id="search-input-icon">
          <i class="fa fa-search"></i>
        </div>
        <input type="search" id="search-input" placeholder="Search...">
      </div>
    </nav>
    <div id="lang-select-wrap">
        <a href="/about/" class="main-nav-link"><label id="lang-select-label"><i class="fa fa-globe"></i><span>关于我</span></label></a>
    </div>
    <a id="mobile-nav-toggle">
      <span class="mobile-nav-toggle-bar"></span>
      <span class="mobile-nav-toggle-bar"></span>
      <span class="mobile-nav-toggle-bar"></span>
    </a>
  </div>
</header>

    <div id="content-wrap">
  <div id="content" class="wrapper">
    <div id="content-inner">
      <article class="article-container" itemscope itemtype="http://schema.org/Article">
        <div class="article-inner">
          <div class="article">
            <div class="inner">
              <header class="article-header">
                <h1 class="article-title" itemprop="name">Hadoop Install & wordcount test</h1>
                <a target="_blank" rel="noopener" href="https://github.com/BoobooWei/site/edit/master/source/database/nosql/hadoop/02_operation/hadoopInstall.md" class="article-edit-link" title="Improve this doc"><i class="fa fa-pencil"></i></a>
              </header>
              <div class="article-content" itemprop="articleBody">
                <html><head></head><body><h2 id="环境介绍" class="article-heading"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍<a class="article-anchor" href="#环境介绍" aria-hidden="true"></a></h2><p>OS:rhel7.2</p>
<p>mastera 172.25.0.11 nameserver(8020 50070) resourcemanager（8032 8030 8088 8031 8033 ）jobhistory(10020 19888)</p>
<p>slavea 172.25.0.13 datanode nodemanager</p>
<p>slaveb 172.25.0.14 datanode nodemanager</p>
<h2 id="初始化环境脚本" class="article-heading"><a href="#初始化环境脚本" class="headerlink" title="初始化环境脚本"></a>初始化环境脚本<a class="article-anchor" href="#初始化环境脚本" aria-hidden="true"></a></h2><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">A1(){</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">对root用户创建无密钥登陆</span></span><br><span class="line">	for i in 11 13 14;do ssh-copy-id root@172.25.0.$i;done</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">A2(){</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">虚拟机创建hadoop用户密码为uplooking</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">虚拟机安装vim wget net-tools</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">真机将公钥拷贝到虚拟机</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">虚拟机hadoop用户新建公钥私钥</span></span><br><span class="line"></span><br><span class="line">	for i in 11 13 14</span><br><span class="line">	do</span><br><span class="line">		ssh root@172.25.0.$i "useradd hadoop ; echo uplooking|passwd --stdin hadoop ; yum install -y 	vim net-tools wget ;systemctl stop firewalld ; setenforce 0;sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config"</span><br><span class="line">		ssh-copy-id hadoop@172.25.0.$i</span><br><span class="line">	done</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">A3(){</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">虚拟机之间无密钥访问，包括自己</span></span><br><span class="line">	for i in 11 13 14</span><br><span class="line">	do</span><br><span class="line">		ssh hadoop@172.25.0.$i "ssh-keygen"</span><br><span class="line">	done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">将虚拟机的公钥保存到真机的tmp.txt中</span></span><br><span class="line">	(for i in 11 13 14;do ssh hadoop@172.25.0.$i "cat /home/hadoop/.ssh/id_rsa.pub";done )&gt; tmp.txt</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">将保存所有虚拟机公钥的文件复制到虚拟机中，并追加到authorized_keys文件中</span></span><br><span class="line">	for i in 11 13 14</span><br><span class="line">	do</span><br><span class="line">		scp tmp.txt  hadoop@172.25.0.$i:~</span><br><span class="line">		ssh hadoop@172.25.0.$i "cat ~/tmp.txt &gt;&gt; /home/hadoop/.ssh/authorized_keys"</span><br><span class="line">	done</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">mastera虚拟机访问自己会产生known_hosts</span></span><br><span class="line">	ssh hadoop@172.25.0.11 "ssh hadoop@172.25.0.11;cat ~/.ssh/known_hosts" &gt; known_hosts.tmp</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">根据known_hosts去写所有虚拟机的known_hosts并保存在真机的known_hosts.tmp文件中</span></span><br><span class="line">	cat &gt; known_hosts.tmp &lt;&lt; ENDF</span><br><span class="line">172.25.0.11 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBL58xGGAGvl4PmE+QXczZ4zmj0OEaaC/jLB0VmiO8ICCIzH825NZrQCWEJAvx+WwEQY7T0cGSvDUzoXOcjr/81c=</span><br><span class="line">172.25.0.13 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBL58xGGAGvl4PmE+QXczZ4zmj0OEaaC/jLB0VmiO8ICCIzH825NZrQCWEJAvx+WwEQY7T0cGSvDUzoXOcjr/81c=</span><br><span class="line">172.25.0.14 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBL58xGGAGvl4PmE+QXczZ4zmj0OEaaC/jLB0VmiO8ICCIzH825NZrQCWEJAvx+WwEQY7T0cGSvDUzoXOcjr/81c=</span><br><span class="line">ENDF</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">将临时文件拷贝到虚拟机即可</span></span><br><span class="line">	for i in 11 13 14;do scp known_hosts.tmp  hadoop@172.25.0.$i:~ ;done</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">A4(){</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">创建文件系统给hadoop使用 20G /hadoop</span></span><br><span class="line">	cat &gt; hadoop_file.sh &lt;&lt; endfile</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">cat &gt; fdisk.tmp &lt;&lt; ENDF</span><br><span class="line">n</span><br><span class="line">p</span><br><span class="line">1</span><br><span class="line">2048</span><br><span class="line">41943039</span><br><span class="line">w</span><br><span class="line">ENDF</span><br><span class="line"></span><br><span class="line">fdisk /dev/vdb &lt; fdisk.tmp</span><br><span class="line">mkfs.xfs /dev/vdb1</span><br><span class="line">mkdir /hadoop</span><br><span class="line">mount /dev/vdb1 /hadoop</span><br><span class="line">chown hadoop. /hadoop/ -R</span><br><span class="line">cat &gt;&gt; /etc/fstab &lt;&lt; ENDF</span><br><span class="line">/dev/vdb1 		/hadoop  		xfs	defaults        0 0</span><br><span class="line">ENDF</span><br><span class="line">mount -a</span><br><span class="line">endfile</span><br><span class="line"></span><br><span class="line">	for i in 11 13 14</span><br><span class="line">	do</span><br><span class="line">		scp hadoop_file.sh  root@172.25.0.$i:~</span><br><span class="line">		ssh  root@172.25.0.$i "bash hadoop_file.sh"</span><br><span class="line">	done</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">A5(){</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">下载hadoop的安装包</span></span><br><span class="line">	ssh hadoop@172.25.0.11 "cd /hadoop ; wget http://classroom.example.com/content/hadoop/cdb5.tar ;wget http://classroom.example.com/content/hadoop/jdk.tar"</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">main(){</span><br><span class="line">A1</span><br><span class="line">A2</span><br><span class="line">A3</span><br><span class="line">A4</span><br><span class="line">A5</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">main</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Install-JDK-Hadoop" class="article-heading"><a href="#Install-JDK-Hadoop" class="headerlink" title="Install JDK &amp; Hadoop"></a>Install JDK &amp; Hadoop<a class="article-anchor" href="#Install-JDK-Hadoop" aria-hidden="true"></a></h2><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">以下操作在mastera上执行</span></span><br><span class="line">B1(){</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">解压缩</span></span><br><span class="line">	cd /hadoop</span><br><span class="line">	tar -xf jdk.tar</span><br><span class="line">	tar -xf cdb5.tar</span><br><span class="line">	rm -rf jdk.tar</span><br><span class="line">	rm -rf cdb5.tar</span><br><span class="line">	rm -rf jdk/jdk-7u79-windows-x64.exe</span><br><span class="line">	cd jdk</span><br><span class="line">	tar -xf jdk-7u79-linux-x64.tar.gz</span><br><span class="line">	rm -rf jdk-7u79-linux-x64.tar.gz</span><br><span class="line">	cd /hadoop/cdb5/</span><br><span class="line">	tar -xf hadoop-2.5.0-cdh5.3.6.tar.gz -C /hadoop</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">安装jdk并宣告java家目录</span></span><br><span class="line">	mv /hadoop/jdk /usr/local/</span><br><span class="line">	echo "export JAVA_HOME=/usr/local/jdk/jdk1.7.0_79/" &gt;&gt; /etc/bashrc</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">安装hadoop，添加hadoop家目录和可执行路径</span></span><br><span class="line">	echo "export HD_HOME=/hadoop/hadoop-2.5.0-cdh5.3.6" &gt;&gt; /etc/bashrc</span><br><span class="line">	echo "export PATH=${PATH}:${HD_HOME}/bin:${HD_HOME}/sbin:${JAVA_HOME}/bin" &gt;&gt; /etc/bashrc</span><br><span class="line">	source /etc/bashrc</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">B1</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Hadoop配置文件" class="article-heading"><a href="#Hadoop配置文件" class="headerlink" title="Hadoop配置文件"></a>Hadoop配置文件<a class="article-anchor" href="#Hadoop配置文件" aria-hidden="true"></a></h2><ul>
<li>core-site.xml模板 share/doc/hadoop-project-dist/hadoop-common/core-default.xml</li>
<li>hdfs-site.xml模板 share/doc/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</li>
<li>yarn-site.xml模板 share/doc/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</li>
<li>mapred-site.xml模板 share/doc/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml</li>
</ul>
<p>列出核心内容</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">B2(){</span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">集群的配置文件</span></span><br><span class="line">	cd ${HD_HOME}/etc/hadoop</span><br><span class="line">	cat &gt; core-site.xml &lt;&lt; ENDF</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://172.25.0.11:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">ENDF</span><br><span class="line"></span><br><span class="line">	cat &gt; hdfs-site.xml &lt;&lt; ENDF</span><br><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:/hadoop/hfs/name&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:/hadoop/hfs/data&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">ENDF</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">yarn-site.xml 中注意两点</span><br><span class="line">yarn.resourcemanager.hostname=172.25.0.11</span><br><span class="line">yarn.nodemanager.aux-services=mapreduce_shuffle</span><br><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line">mapreduce.framework.name=yarn</span><br><span class="line"></span><br><span class="line">echo mastera0 &gt; masters</span><br><span class="line">echo slavea0 &gt; slaves</span><br><span class="line">echo slaveb0 &gt;&gt; slaves</span><br><span class="line"></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">C1(){</span><br><span class="line">for i in 13 14 ; do scp -r  root@172.25.0.11:/usr/local/jdk root@172.25.0.$i:/usr/local;done</span><br><span class="line">for i in 13 14 ; do scp -r  hadoop@172.25.0.11:/hadoop/hadoop-2.5.0-cdh5.3.6 hadoop@172.25.0.$i:/hadoop;done</span><br><span class="line">for i in 11 13 14 ; do ssh root@172.25.0.$i "chown hadoop. /hadoop -R";done</span><br><span class="line">for i in 13 14 ; do scp -r  root@172.25.0.11:/etc/bashrc root@172.25.0.$i:/etc/bashrc;done</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">B1</span><br><span class="line">B2</span><br><span class="line">C1</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h2 id="wordcount-测试" class="article-heading"><a href="#wordcount-测试" class="headerlink" title="wordcount 测试"></a>wordcount 测试<a class="article-anchor" href="#wordcount-测试" aria-hidden="true"></a></h2><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">格式化namenode</span></span><br><span class="line">hdfs namenode -format</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动hdfs和mapreduce（yarn）</span></span><br><span class="line">start-dfs.sh</span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">start-all.sh</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动hisotryserver</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">sbin/mr-jobhistory-daemon.sh stop historyserver</span></span><br><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看java进程</span></span><br><span class="line">jps</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">第归创建测试目录</span></span><br><span class="line">hdfs dfs -mkdir -p /booboo/input</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将单词文件放入测试目录</span></span><br><span class="line"><span class="meta prompt_">cat&gt; </span><span class="language-bash">words &lt;&lt; <span class="string">ENDF</span></span></span><br><span class="line">hello tom</span><br><span class="line">hello jack</span><br><span class="line">hello superman</span><br><span class="line">superman is me</span><br><span class="line">batman vs superman</span><br><span class="line">ENDF</span><br><span class="line"></span><br><span class="line">hdfs dfs -put words /booboo/input</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">开始测试，并将结果输出到制定目录</span></span></span><br><span class="line">yarn jar hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar wordcount /booboo/input /booboo/output1</span><br></pre></td></tr></tbody></table></figure>

<h2 id="详细操作留档" class="article-heading"><a href="#详细操作留档" class="headerlink" title="详细操作留档"></a>详细操作留档<a class="article-anchor" href="#详细操作留档" aria-hidden="true"></a></h2><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">[hadoop@mastera0 ~]$ hdfs namenode -format</span><br><span class="line">17/04/05 14:48:22 INFO namenode.NameNode: STARTUP_MSG:</span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = mastera0.example.com/172.25.0.11</span><br><span class="line">STARTUP_MSG:   args = [-format]</span><br><span class="line">STARTUP_MSG:   version = 2.5.0-cdh5.3.6</span><br><span class="line">STARTUP_MSG:   classpath = /hadoop/hadoop-2.5.0-cdh5.3.6/etc/hadoop:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-cli-1.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/guava-11.0.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jsch-0.1.42.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-digester-1.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/xz-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jettison-1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jetty-util-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jsp-api-2.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-collections-3.2.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/xmlenc-0.52.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jersey-core-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/asm-3.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-el-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-codec-1.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jersey-json-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-lang-2.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/curator-framework-2.6.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jsr305-1.3.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/gson-2.2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/paranamer-2.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/avro-1.7.6-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-net-3.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/junit-4.11.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/activation-1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/servlet-api-2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/hadoop-annotations-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jersey-server-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/log4j-1.2.17.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/zookeeper-3.4.5-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/curator-client-2.6.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jetty-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-io-2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/hadoop-nfs-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.6-tests.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/asm-3.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-el-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jetty-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.6-tests.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jline-0.9.94.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/xz-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jettison-1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/asm-3.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/guice-3.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/javax.inject-1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/activation-1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/zookeeper-3.4.5-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jetty-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-client-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-common-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-api-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-common-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/avro-1.7.6-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.6-tests.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.0-cdh5.3.6.jar:/contrib/capacity-scheduler/*.jar</span><br><span class="line">STARTUP_MSG:   build = http://github.com/cloudera/hadoop -r 6743ef286bfdd317b600adbdb154f982cf2fac7a; compiled by 'jenkins' on 2015-07-28T22:14Z</span><br><span class="line">STARTUP_MSG:   java = 1.7.0_79</span><br><span class="line">************************************************************/</span><br><span class="line">17/04/05 14:48:22 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]</span><br><span class="line">17/04/05 14:48:22 INFO namenode.NameNode: createNameNode [-format]</span><br><span class="line">17/04/05 14:48:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Formatting using clusterid: CID-954f4053-15e7-4bda-b7de-18f0cf9397f8</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: No KeyProvider found.</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: fsLock is fair:true</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: The block deletion will start around 2017 Apr 05 14:48:23</span><br><span class="line">17/04/05 14:48:23 INFO util.GSet: Computing capacity for map BlocksMap</span><br><span class="line">17/04/05 14:48:23 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">17/04/05 14:48:23 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB</span><br><span class="line">17/04/05 14:48:23 INFO util.GSet: capacity      = 2^21 = 2097152 entries</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: defaultReplication         = 2</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: maxReplication             = 512</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: minReplication             = 1</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: encryptDataTransfer        = false</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: supergroup          = supergroup</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: isPermissionEnabled = true</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: HA Enabled: false</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: Append Enabled: true</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: Computing capacity for map INodeMap</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: capacity      = 2^20 = 1048576 entries</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NameNode: Caching file names occuring more than 10 times</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: Computing capacity for map cachedBlocks</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: capacity      = 2^18 = 262144 entries</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: Retry cache on namenode is enabled</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: Computing capacity for map NameNodeRetryCache</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: capacity      = 2^15 = 32768 entries</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NNConf: ACLs enabled? false</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NNConf: XAttrs enabled? true</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NNConf: Maximum size of an xattr: 16384</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSImage: Allocated new BlockPoolId: BP-885559910-172.25.0.11-1491374904248</span><br><span class="line">17/04/05 14:48:24 INFO common.Storage: Storage directory /hadoop/hfs/name has been successfully formatted.</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">17/04/05 14:48:24 INFO util.ExitUtil: Exiting with status 0</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NameNode: SHUTDOWN_MSG:</span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at mastera0.example.com/172.25.0.11</span><br><span class="line">************************************************************/</span><br><span class="line"></span><br><span class="line">[hadoop@mastera0 hadoop]$ start-all.sh</span><br><span class="line">[hadoop@mastera0 hadoop]$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">[hadoop@mastera0 hadoop]$ hdfs dfs -mkdir -p /booboo/input</span><br><span class="line">17/04/05 15:18:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@mastera0 hadoop]$ ls</span><br><span class="line">cdb5  hadoop-2.5.0-cdh5.3.6  hfs</span><br><span class="line">[hadoop@mastera0 hadoop]$ touch words</span><br><span class="line">[hadoop@mastera0 hadoop]$ vim words</span><br><span class="line">[hadoop@mastera0 hadoop]$ cat words</span><br><span class="line">hello tom</span><br><span class="line">hello jack</span><br><span class="line">hello superman</span><br><span class="line">superman is me</span><br><span class="line">batman vs superman</span><br><span class="line"></span><br><span class="line">[hadoop@mastera0 hadoop]$ hdfs dfs -put words /booboo/input</span><br><span class="line">17/04/05 15:19:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line"></span><br><span class="line">[hadoop@mastera0 hadoop]$ yarn jar hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar wordcount /booboo/input /booboo/output1</span><br><span class="line">17/04/05 15:21:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">17/04/05 15:21:50 INFO client.RMProxy: Connecting to ResourceManager at /172.25.0.11:8032</span><br><span class="line">17/04/05 15:21:51 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">17/04/05 15:21:51 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">17/04/05 15:21:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1491376466525_0001</span><br><span class="line">17/04/05 15:21:52 INFO impl.YarnClientImpl: Submitted application application_1491376466525_0001</span><br><span class="line">17/04/05 15:21:52 INFO mapreduce.Job: The url to track the job: http://mastera0.example.com:8088/proxy/application_1491376466525_0001/</span><br><span class="line">17/04/05 15:21:52 INFO mapreduce.Job: Running job: job_1491376466525_0001</span><br><span class="line">17/04/05 15:22:02 INFO mapreduce.Job: Job job_1491376466525_0001 running in uber mode : false</span><br><span class="line">17/04/05 15:22:02 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">17/04/05 15:22:10 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">17/04/05 15:22:16 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">17/04/05 15:22:17 INFO mapreduce.Job: Job job_1491376466525_0001 completed successfully</span><br><span class="line">17/04/05 15:22:17 INFO mapreduce.Job: Counters: 49</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=94</span><br><span class="line">		FILE: Number of bytes written=206243</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=177</span><br><span class="line">		HDFS: Number of bytes written=56</span><br><span class="line">		HDFS: Number of read operations=6</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">	Job Counters</span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=1</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=5502</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=4424</span><br><span class="line">		Total time spent by all map tasks (ms)=5502</span><br><span class="line">		Total time spent by all reduce tasks (ms)=4424</span><br><span class="line">		Total vcore-seconds taken by all map tasks=5502</span><br><span class="line">		Total vcore-seconds taken by all reduce tasks=4424</span><br><span class="line">		Total megabyte-seconds taken by all map tasks=5634048</span><br><span class="line">		Total megabyte-seconds taken by all reduce tasks=4530176</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=5</span><br><span class="line">		Map output records=12</span><br><span class="line">		Map output bytes=118</span><br><span class="line">		Map output materialized bytes=94</span><br><span class="line">		Input split bytes=107</span><br><span class="line">		Combine input records=12</span><br><span class="line">		Combine output records=8</span><br><span class="line">		Reduce input groups=8</span><br><span class="line">		Reduce shuffle bytes=94</span><br><span class="line">		Reduce input records=8</span><br><span class="line">		Reduce output records=8</span><br><span class="line">		Spilled Records=16</span><br><span class="line">		Shuffled Maps =1</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=1</span><br><span class="line">		GC time elapsed (ms)=42</span><br><span class="line">		CPU time spent (ms)=1550</span><br><span class="line">		Physical memory (bytes) snapshot=431570944</span><br><span class="line">		Virtual memory (bytes) snapshot=1838178304</span><br><span class="line">		Total committed heap usage (bytes)=273678336</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">[hadoop@mastera0 ~]$ hdfs namenode -format</span><br><span class="line">17/04/05 14:48:22 INFO namenode.NameNode: STARTUP_MSG:</span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = mastera0.example.com/172.25.0.11</span><br><span class="line">STARTUP_MSG:   args = [-format]</span><br><span class="line">STARTUP_MSG:   version = 2.5.0-cdh5.3.6</span><br><span class="line">STARTUP_MSG:   classpath = /hadoop/hadoop-2.5.0-cdh5.3.6/etc/hadoop:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-cli-1.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/guava-11.0.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jsch-0.1.42.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-digester-1.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jets3t-0.9.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-configuration-1.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-compress-1.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/xz-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jettison-1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jetty-util-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/httpcore-4.2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/netty-3.6.2.Final.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jsp-api-2.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-collections-3.2.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/xmlenc-0.52.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jersey-core-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-httpclient-3.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/asm-3.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-el-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-codec-1.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jersey-json-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-lang-2.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/stax-api-1.0-2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/curator-framework-2.6.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jsr305-1.3.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/hamcrest-core-1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/gson-2.2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/paranamer-2.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/avro-1.7.6-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-net-3.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/junit-4.11.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/mockito-all-1.8.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/activation-1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/servlet-api-2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/hadoop-annotations-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jersey-server-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/httpclient-4.2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/log4j-1.2.17.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/zookeeper-3.4.5-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/curator-client-2.6.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/jetty-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/lib/commons-io-2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/hadoop-nfs-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.6-tests.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/guava-11.0.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/asm-3.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-el-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/jetty-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/lib/commons-io-2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.6-tests.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jline-0.9.94.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-cli-1.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/guava-11.0.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/xz-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jettison-1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-client-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-core-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/asm-3.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-codec-1.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-json-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-lang-2.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/guice-3.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/javax.inject-1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/activation-1.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/servlet-api-2.5.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jersey-server-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/log4j-1.2.17.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/zookeeper-3.4.5-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/jetty-6.1.26.cloudera.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/lib/commons-io-2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-client-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-common-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-api-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-common-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/xz-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/asm-3.2.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/guice-3.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/javax.inject-1.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/avro-1.7.6-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/junit-4.11.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.6-tests.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.0-cdh5.3.6.jar:/hadoop/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.0-cdh5.3.6.jar:/contrib/capacity-scheduler/*.jar</span><br><span class="line">STARTUP_MSG:   build = http://github.com/cloudera/hadoop -r 6743ef286bfdd317b600adbdb154f982cf2fac7a; compiled by 'jenkins' on 2015-07-28T22:14Z</span><br><span class="line">STARTUP_MSG:   java = 1.7.0_79</span><br><span class="line">************************************************************/</span><br><span class="line">17/04/05 14:48:22 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]</span><br><span class="line">17/04/05 14:48:22 INFO namenode.NameNode: createNameNode [-format]</span><br><span class="line">17/04/05 14:48:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Formatting using clusterid: CID-954f4053-15e7-4bda-b7de-18f0cf9397f8</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: No KeyProvider found.</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: fsLock is fair:true</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: The block deletion will start around 2017 Apr 05 14:48:23</span><br><span class="line">17/04/05 14:48:23 INFO util.GSet: Computing capacity for map BlocksMap</span><br><span class="line">17/04/05 14:48:23 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">17/04/05 14:48:23 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB</span><br><span class="line">17/04/05 14:48:23 INFO util.GSet: capacity      = 2^21 = 2097152 entries</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: defaultReplication         = 2</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: maxReplication             = 512</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: minReplication             = 1</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: encryptDataTransfer        = false</span><br><span class="line">17/04/05 14:48:23 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: supergroup          = supergroup</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: isPermissionEnabled = true</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: HA Enabled: false</span><br><span class="line">17/04/05 14:48:23 INFO namenode.FSNamesystem: Append Enabled: true</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: Computing capacity for map INodeMap</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: capacity      = 2^20 = 1048576 entries</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NameNode: Caching file names occuring more than 10 times</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: Computing capacity for map cachedBlocks</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: capacity      = 2^18 = 262144 entries</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: Retry cache on namenode is enabled</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: Computing capacity for map NameNodeRetryCache</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB</span><br><span class="line">17/04/05 14:48:24 INFO util.GSet: capacity      = 2^15 = 32768 entries</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NNConf: ACLs enabled? false</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NNConf: XAttrs enabled? true</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NNConf: Maximum size of an xattr: 16384</span><br><span class="line">17/04/05 14:48:24 INFO namenode.FSImage: Allocated new BlockPoolId: BP-885559910-172.25.0.11-1491374904248</span><br><span class="line">17/04/05 14:48:24 INFO common.Storage: Storage directory /hadoop/hfs/name has been successfully formatted.</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">17/04/05 14:48:24 INFO util.ExitUtil: Exiting with status 0</span><br><span class="line">17/04/05 14:48:24 INFO namenode.NameNode: SHUTDOWN_MSG:</span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at mastera0.example.com/172.25.0.11</span><br><span class="line">************************************************************/</span><br><span class="line"></span><br><span class="line">[hadoop@mastera0 hadoop]$ start-all.sh</span><br><span class="line">[hadoop@mastera0 hadoop]$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">[hadoop@mastera0 hadoop]$ hdfs dfs -mkdir -p /booboo/input</span><br><span class="line">17/04/05 15:18:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">[hadoop@mastera0 hadoop]$ ls</span><br><span class="line">cdb5  hadoop-2.5.0-cdh5.3.6  hfs</span><br><span class="line">[hadoop@mastera0 hadoop]$ touch words</span><br><span class="line">[hadoop@mastera0 hadoop]$ vim words</span><br><span class="line">[hadoop@mastera0 hadoop]$ cat words</span><br><span class="line">hello tom</span><br><span class="line">hello jack</span><br><span class="line">hello superman</span><br><span class="line">superman is me</span><br><span class="line">batman vs superman</span><br><span class="line"></span><br><span class="line">[hadoop@mastera0 hadoop]$ hdfs dfs -put words /booboo/input</span><br><span class="line">17/04/05 15:19:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line"></span><br><span class="line">[hadoop@mastera0 hadoop]$ yarn jar hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar wordcount /booboo/input /booboo/output1</span><br><span class="line">17/04/05 15:21:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">17/04/05 15:21:50 INFO client.RMProxy: Connecting to ResourceManager at /172.25.0.11:8032</span><br><span class="line">17/04/05 15:21:51 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">17/04/05 15:21:51 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">17/04/05 15:21:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1491376466525_0001</span><br><span class="line">17/04/05 15:21:52 INFO impl.YarnClientImpl: Submitted application application_1491376466525_0001</span><br><span class="line">17/04/05 15:21:52 INFO mapreduce.Job: The url to track the job: http://mastera0.example.com:8088/proxy/application_1491376466525_0001/</span><br><span class="line">17/04/05 15:21:52 INFO mapreduce.Job: Running job: job_1491376466525_0001</span><br><span class="line">17/04/05 15:22:02 INFO mapreduce.Job: Job job_1491376466525_0001 running in uber mode : false</span><br><span class="line">17/04/05 15:22:02 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">17/04/05 15:22:10 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">17/04/05 15:22:16 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">17/04/05 15:22:17 INFO mapreduce.Job: Job job_1491376466525_0001 completed successfully</span><br><span class="line">17/04/05 15:22:17 INFO mapreduce.Job: Counters: 49</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=94</span><br><span class="line">		FILE: Number of bytes written=206243</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=177</span><br><span class="line">		HDFS: Number of bytes written=56</span><br><span class="line">		HDFS: Number of read operations=6</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">	Job Counters</span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=1</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=5502</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=4424</span><br><span class="line">		Total time spent by all map tasks (ms)=5502</span><br><span class="line">		Total time spent by all reduce tasks (ms)=4424</span><br><span class="line">		Total vcore-seconds taken by all map tasks=5502</span><br><span class="line">		Total vcore-seconds taken by all reduce tasks=4424</span><br><span class="line">		Total megabyte-seconds taken by all map tasks=5634048</span><br><span class="line">		Total megabyte-seconds taken by all reduce tasks=4530176</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=5</span><br><span class="line">		Map output records=12</span><br><span class="line">		Map output bytes=118</span><br><span class="line">		Map output materialized bytes=94</span><br><span class="line">		Input split bytes=107</span><br><span class="line">		Combine input records=12</span><br><span class="line">		Combine output records=8</span><br><span class="line">		Reduce input groups=8</span><br><span class="line">		Reduce shuffle bytes=94</span><br><span class="line">		Reduce input records=8</span><br><span class="line">		Reduce output records=8</span><br><span class="line">		Spilled Records=16</span><br><span class="line">		Shuffled Maps =1</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=1</span><br><span class="line">		GC time elapsed (ms)=42</span><br><span class="line">		CPU time spent (ms)=1550</span><br><span class="line">		Physical memory (bytes) snapshot=431570944</span><br><span class="line">		Virtual memory (bytes) snapshot=1838178304</span><br><span class="line">		Total committed heap usage (bytes)=273678336</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters</span><br><span class="line">		Bytes Read=70</span><br><span class="line">	File Output Format Counters</span><br><span class="line">		Bytes Written=56</span><br><span class="line">[hadoop@mastera0 hadoop]$ hdfs dfs -cat /booboo/output1/part-r-00000</span><br><span class="line">17/04/05 15:25:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">batman	1</span><br><span class="line">hello	3</span><br><span class="line">is	1</span><br><span class="line">jack	1</span><br><span class="line">me	1</span><br><span class="line">superman	3</span><br><span class="line">tom	1</span><br><span class="line">vs	1</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters</span><br><span class="line">		Bytes Read=70</span><br><span class="line">	File Output Format Counters</span><br><span class="line">		Bytes Written=56</span><br><span class="line">[hadoop@mastera0 hadoop]$ hdfs dfs -cat /booboo/output1/part-r-00000</span><br><span class="line">17/04/05 15:25:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">batman	1</span><br><span class="line">hello	3</span><br><span class="line">is	1</span><br><span class="line">jack	1</span><br><span class="line">me	1</span><br><span class="line">superman	3</span><br><span class="line">tom	1</span><br><span class="line">vs	1</span><br></pre></td></tr></tbody></table></figure>

<p><img src="pic/01.png" alt="01"><br><img src="pic/02.png" alt="01"><br><img src="pic/03.png" alt="01"><br><img src="pic/04.png" alt="01"><br><img src="pic/05.png" alt="01"><br><img src="pic/06.png" alt="01"><br><img src="pic/07.png" alt="01"><br><img src="pic/08.png" alt="01"><br><img src="pic/09.png" alt="01"><br><img src="pic/10.png" alt="01"></p>
</body></html>
              </div>
              <footer class="article-footer">
                <time class="article-footer-updated" datetime="2025-10-22T08:49:36.787Z" itemprop="dateModified">Last updated: 2025-10-22</time>
                <a href="/database/mysql/index.html" class="article-footer-next" title="概览"><span>Next</span><i class="fa fa-chevron-right"></i></a>
              </footer>
              
            </div>
          </div>
          <aside id="article-toc" role="navigation">
            <div id="article-toc-inner">
              <div id="carbonads" class="carbonads" style="text-align:center;font-family:'Noto Sans SC',sans-serif;">
  <span class="carbon-wrap">
    <a href="https://github.com/booboowei"
       class="carbon-img"
       target="_blank"
       rel="noopener sponsored">
      <img
        src="https://avatars2.githubusercontent.com/u/21328020?s=460&u=88cf6127c32932188f936d05636b7b0d36783ee1&v=4"
        alt="ads via Carbon"
        border="0"
        style="width:80px;height:80px;border-radius:50%;max-width:130px;"
      />
    </a>
    <a href="/about/"
       class="carbon-text"
       target="_blank"
       rel="noopener sponsored"
       style="display:block;margin-top:4px;">
      欢迎光临
    </a>
  </span>

  <!-- 日期显示 -->
  <div id="today-date" style="margin-top:12px;background:#f5f5f5;border-radius:12px;padding:10px 0;">
    <div id="weekday" style="font-size:14px;color:#555;"></div>
    <div id="day" style="font-size:36px;font-weight:bold;color:#000;"></div>
    <div id="ym" style="font-size:14px;color:#555;"></div>
    <div id="lunar" style="font-size:13px;color:#888;margin-top:4px;"></div>
  </div>

<script>
(function(){
  const now = new Date();
  const weekdays = ["星期日","星期一","星期二","星期三","星期四","星期五","星期六"];
  const year = now.getFullYear();
  const month = now.getMonth() + 1;
  const day = now.getDate();

  function toChineseNumber(num) {
    const map = {0:'〇',1:'一',2:'二',3:'三',4:'四',5:'五',6:'六',7:'七',8:'八',9:'九'};
    return String(num).split('').map(n=>map[n]).join('');
  }

  // 优先使用 Intl (现代浏览器支持农历/中国传统历)
  function getLunarByIntl(date) {
    try {
      // 获取农历的月（长格式，如 "九月" 或 "闰八月"），和日（数字）
      const monthFormatter = new Intl.DateTimeFormat('zh-CN-u-ca-chinese', { month: 'long' });
      const dayFormatter = new Intl.DateTimeFormat('zh-CN-u-ca-chinese', { day: 'numeric' });
      const lunarMonthText = monthFormatter.format(date); // 例如 "九月" 或 "闰八月"
      const lunarDayText = dayFormatter.format(date);     // 例如 "三十" 或 "初一"
      // 有的浏览器返回的 month 会包含 "闰" 字样，也可能返回汉字月份，需要适配
      return `农历${lunarMonthText}${lunarDayText}`;
    } catch (e) {
      return null; // 表示不支持 Intl 农历
    }
  }

  // 回退：简易本地农历算法（仅作兼容，适用于1900-2049范围）
  const lunarInfo = [0x04bd8,0x04ae0,0x0a570,0x054d5,0x0d260,0x0d950,0x16554,0x056a0,0x09ad0,0x055d2,
                     0x04ae0,0x0a5b6,0x0a4d0,0x0d250,0x1d255,0x0b540,0x0d6a0,0x0ada2,0x095b0,0x14977,
                     0x04970,0x0a4b0,0x0b4b5,0x06a50,0x06d40,0x1ab54,0x02b60,0x09570,0x052f2,0x04970,
                     0x06566,0x0d4a0,0x0ea50,0x06e95,0x05ad0,0x02b60,0x186e3,0x092e0,0x1c8d7,0x0c950,
                     0x0d4a0,0x1d8a6,0x0b550,0x056a0,0x1a5b4,0x025d0,0x092d0,0x0d2b2,0x0a950,0x0b557,
                     0x06ca0,0x0b550,0x15355,0x04da0,0x0a5b0,0x14573,0x052b0,0x0a9a8,0x0e950,0x06aa0,
                     0x0aea6,0x0ab50,0x04b60,0x0aae4,0x0a570,0x05260,0x0f263,0x0d950,0x05b57,0x056a0,
                     0x096d0,0x04dd5,0x04ad0,0x0a4d0,0x0d4d4,0x0d250,0x0d558,0x0b540,0x0b5a0,0x195a6,
                     0x095b0,0x049b0,0x0a974,0x0a4b0,0x0b27a,0x06a50,0x06d40,0x0af46,0x0ab60,0x09570,
                     0x04af5,0x04970,0x064b0,0x074a3,0x0ea50,0x06b58,0x05ac0,0x0ab60,0x096d5,0x092e0];

  function lYearDays(y){let i,sum=348;for(i=0x8000;i>0x8;i>>=1)sum+=(lunarInfo[y-1900]&i)?1:0;return sum+leapDays(y);}
  function leapDays(y){if(leapMonth(y))return(lunarInfo[y-1900]&0x10000)?30:29;else return 0;}
  function leapMonth(y){return lunarInfo[y-1900]&0xf;}
  function monthDays(y,m){return(lunarInfo[y-1900]&(0x10000>>m))?30:29;}

  function LunarFallback(objDate){
    let i, leap=0, temp=0;
    const baseDate = new Date(1900,0,31);
    let offset = Math.floor((objDate - baseDate) / 86400000);
    let year = 1900;
    // 年循环
    for (i = 1900; i < 2050 && offset > 0; i++) {
      temp = lYearDays(i);
      offset -= temp;
      year = i;
    }
    if (offset < 0) { offset += temp; year--; }
    leap = leapMonth(year);
    let isLeap = false;
    let month = 1;
    // 月循环
    for (i = 1; i <= 12 && offset > 0; i++) {
      if (leap > 0 && i === (leap + 1) && !isLeap) {
        // 进入闰月
        temp = leapDays(year);
        isLeap = true;
        i--; // stay on same month number but mark leap
      } else {
        temp = monthDays(year, i);
      }
      offset -= temp;
      if (isLeap && i === leap) { isLeap = false; }
      if (!isLeap) month++; // 非闰月时才推进月份计数
    }
    if (offset === 0 && leap > 0 && month === leap + 1) {
      // 特殊：当正好落在闰月第一天
      // month 保持，isLeap = true
    }
    if (offset < 0) { offset += temp; month--; }
    const day = offset + 1;
    // 修正 month 边界
    if (month <= 0) month = 1;
    return { year, month, day, isLeap: !!isLeap };
  }

  function getLunarDateStringFallback(y,m,d){
    const cDay=["","初一","初二","初三","初四","初五","初六","初七","初八","初九","初十",
                "十一","十二","十三","十四","十五","十六","十七","十八","十九","二十",
                "廿一","廿二","廿三","廿四","廿五","廿六","廿七","廿八","廿九","三十"];
    const cMon=["","正月","二月","三月","四月","五月","六月","七月","八月","九月","十月","冬月","腊月"];
    const lunar = LunarFallback(new Date(y, m-1, d));
    return `农历${lunar.isLeap ? '闰' : ''}${cMon[lunar.month]}${cDay[lunar.day]}`;
  }

  // 先尝试 Intl 方法
  let lunarText = getLunarByIntl(now);
  if (!lunarText) {
    // 回退本地算法
    lunarText = getLunarDateStringFallback(year, month, day);
  }

  // 输出到页面（公历 + 农历）
  document.getElementById("weekday").textContent = weekdays[now.getDay()];
  document.getElementById("day").textContent = day;
  document.getElementById("ym").textContent = `${toChineseNumber(year)}年${toChineseNumber(month)}月`;
  document.getElementById("lunar").textContent = lunarText;
})();
</script>


              <strong class="sidebar-title">Contents</strong>
              <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E4%BB%8B%E7%BB%8D"><span class="toc-text">环境介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E7%8E%AF%E5%A2%83%E8%84%9A%E6%9C%AC"><span class="toc-text">初始化环境脚本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Install-JDK-Hadoop"><span class="toc-text">Install JDK &amp; Hadoop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">Hadoop配置文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#wordcount-%E6%B5%8B%E8%AF%95"><span class="toc-text">wordcount 测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E6%93%8D%E4%BD%9C%E7%95%99%E6%A1%A3"><span class="toc-text">详细操作留档</span></a></li></ol>
              <a href="#" id="article-toc-top">Back to Top</a>
            </div>
          </aside>
        </div>
      </article>
      <aside id="sidebar" role="navigation">
  <div class="inner">
    <strong class="sidebar-title">MySQL</strong><a href="/database/mysql/index.html" class="sidebar-link">概览</a><a href="/database/mysql/booboo_mysql/index.html" class="sidebar-link">MySQL 基础</a><a href="/database/mysql/dba_mysql/index.html" class="sidebar-link">MySQL 运维实践</a><a href="/database/mysql/rds_mysql/index.html" class="sidebar-link">RDS 性能调优</a><a href="/database/mysql/security/index.html" class="sidebar-link">安全篇</a><a href="/database/mysql/awesome-tools/index.html" class="sidebar-link">工具篇</a><strong class="sidebar-title">Oracle</strong><a href="/database/oracle/index.html" class="sidebar-link">概览</a><a href="/database/oracle/ocp/index.html" class="sidebar-link">Oracle12C OCP</a><a href="/database/oracle/oracle-12c/index.html" class="sidebar-link">Oracle 12c 学习笔记</a><a href="/database/oracle/oracle-11g/index.html" class="sidebar-link">Oracle 11g 学习笔记</a><strong class="sidebar-title">NoSQL</strong><a href="/database/nosql/index.html" class="sidebar-link">概览</a><a href="/database/nosql/booboo_redis/index.html" class="sidebar-link">Redis 基础</a><a href="/database/nosql/dba_redis/index.html" class="sidebar-link">Redis 进阶</a><a href="/database/nosql/booboo_mongodb/index.html" class="sidebar-link">MongoDB 基础</a><a href="/database/nosql/hadoop/index.html" class="sidebar-link">Hadoop 基础</a><strong class="sidebar-title">TiDB</strong><a href="/database/tidb/index.html" class="sidebar-link">概览</a><a href="/database/tidb/00_tidb入门指南.html" class="sidebar-link">TiDB 入门指南</a><a href="/database/tidb/01_raft协议理解.html" class="sidebar-link">Raft协议理解</a><a href="/database/tidb/tidb_courses/index.html" class="sidebar-link">TiDB 21天课程</a>
  </div>
</aside>
    </div>
  </div>
</div>

    <footer id="footer" class="wrapper">
  <div class="inner">
    <div id="footer-copyright">
      &copy; 2025 <a href="https://github.com/BoobooWei" target="_blank">魏亚萍</a><br>
      Documentation licensed under <a href="http://creativecommons.org/licenses/by/4.0/" target="_blank">CC BY 4.0</a>.<br>
      备案号：沪ICP备2020026043号
    </div>
    <div id="footer-links">
      <a href="https://www.youtube.com/@amyreading2023" class="footer-link" target="_blank">
        <img src="https://upload.wikimedia.org/wikipedia/commons/b/b8/YouTube_Logo_2017.svg" 
            alt="YouTube"
            style="height:24px; margin-bottom:-6px;">
      </a>
      <a href="https://github.com/BoobooWei" class="footer-link" target="_blank"><i class="fa fa-github-alt"></i></a>
    </div>
  </div>
</footer>

  </div>
  <div id="mobile-nav-dimmer"></div>
  <nav id="mobile-nav">
  <div id="mobile-nav-inner">
    <ul id="mobile-nav-list">
      <a href="../../../../docs/" class="mobile-nav-link">Hexo</a><a href="../../../../news/" class="mobile-nav-link">News</a><a href="../../../../api/" class="mobile-nav-link">MySQL8.0</a><a href="../../../../database/" class="mobile-nav-link">Database</a><a href="../../../../linux/" class="mobile-nav-link">Linux</a><a href="../../../../cloud/" class="mobile-nav-link">Cloud</a><a href="../../../../singapore/" class="mobile-nav-link">Singapore</a><a href="../../../../amy/" class="mobile-nav-link">AMY</a><a href="../../../../about/" class="mobile-nav-link">About</a>
      <li class="mobile-nav-item">
        <a href="https://github.com/BoobooWei" class="mobile-nav-link" rel="external" target="_blank">GitHub</a>
      </li>
    </ul>
    
      <strong class="mobile-nav-title">MySQL</strong><a href="/database/mysql/index.html" class="mobile-nav-link">概览</a><a href="/database/mysql/booboo_mysql/index.html" class="mobile-nav-link">MySQL 基础</a><a href="/database/mysql/dba_mysql/index.html" class="mobile-nav-link">MySQL 运维实践</a><a href="/database/mysql/rds_mysql/index.html" class="mobile-nav-link">RDS 性能调优</a><a href="/database/mysql/security/index.html" class="mobile-nav-link">安全篇</a><a href="/database/mysql/awesome-tools/index.html" class="mobile-nav-link">工具篇</a><strong class="mobile-nav-title">Oracle</strong><a href="/database/oracle/index.html" class="mobile-nav-link">概览</a><a href="/database/oracle/ocp/index.html" class="mobile-nav-link">Oracle12C OCP</a><a href="/database/oracle/oracle-12c/index.html" class="mobile-nav-link">Oracle 12c 学习笔记</a><a href="/database/oracle/oracle-11g/index.html" class="mobile-nav-link">Oracle 11g 学习笔记</a><strong class="mobile-nav-title">NoSQL</strong><a href="/database/nosql/index.html" class="mobile-nav-link">概览</a><a href="/database/nosql/booboo_redis/index.html" class="mobile-nav-link">Redis 基础</a><a href="/database/nosql/dba_redis/index.html" class="mobile-nav-link">Redis 进阶</a><a href="/database/nosql/booboo_mongodb/index.html" class="mobile-nav-link">MongoDB 基础</a><a href="/database/nosql/hadoop/index.html" class="mobile-nav-link">Hadoop 基础</a><strong class="mobile-nav-title">TiDB</strong><a href="/database/tidb/index.html" class="mobile-nav-link">概览</a><a href="/database/tidb/00_tidb入门指南.html" class="mobile-nav-link">TiDB 入门指南</a><a href="/database/tidb/01_raft协议理解.html" class="mobile-nav-link">Raft协议理解</a><a href="/database/tidb/tidb_courses/index.html" class="mobile-nav-link">TiDB 21天课程</a>
    
  </div>
  <div id="mobile-lang-select-wrap">
    <span id="mobile-lang-select-label"><i class="fa fa-globe"></i><span>English</span></span>
    <select id="mobile-lang-select" data-canonical="">
      
        <option value="en" selected>English</option>
      
    </select>
  </div>
</nav>
  <!-- Scripts -->
<!-- Cookie -->
<script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script>
<!-- build:js build/js/main.js -->

<script src="../../../../js/lang_select.js"></script>


<script src="../../../../js/mobile_nav.js"></script>

<!-- endbuild -->

<!-- Algolia -->
<!--   backup
<script src="https://cdn.jsdelivr.net/npm/@docsearch/js@3"></script>
<script type="text/javascript">
docsearch({
  appId: '',
  apiKey: '',
  indexName: '',
  insights: true,
  container: '#docsearch',
  debug: false,
  searchParameters: {
    facetFilters: ['lang:en']
  }
});
</script>
-->

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
<script type="text/javascript">
document.getElementById('search-input-wrap').classList.add('on');
docsearch({
    apiKey: '',
    indexName: '',
    inputSelector: '#search-input'
});
</script>
</body>
</html>